# Minimizing Hallucination in Truth Grounded Scenarios

## Use Cases

Hallunications are prompt responses that are not based on facts or diffr contextualy, these responses can be minor deviations to completely false or divergent outputs.

The Large language models are used to drive customer insights on use cases not limited to Summarization of text, Automated response generation, Content Generation, Code generation, Code Documentation, Semnatic Search with Embeddings, Information Discovery and Knowledge mining. These uses cases drive business value across various vertical use cases and industries including legal, retail, customer services, call center, entertainment, financial, banking and travel industries. 
Prompt input governance, control and best practices play a significant role in the model output quality and in controlling the model Hallucination

## Challenges

[ why is this difficult to do? what should be considered to solve the problem? ]

## Solution Patterns

---
### Pattern 1: (name it)
---
#### Approach

[ describe the approach ]

#### Implementation

[ this does not need to be a fully working implementation, but should provide enough information to understand the approach]

[ provide pseudo code or notebooks, etc. in 'src' directory ]

[ include architecture diagrams in 'assets' directory as needed to illustrate the approach ]

#### Performance

[ discuss / evaluate the performance of the approach (accuracy, speed, etc.) for instance here, use Rouge() or BERTScore()]

#### Strengths

[ discuss the strengths of the approach ]

#### Limitations

[ when does this approach fails or is not recommended ]

---
### Pattern 2: (name it)
---
[ ... ]
